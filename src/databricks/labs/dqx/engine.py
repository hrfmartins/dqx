import functools as ft
import itertools
import re
from collections.abc import Callable
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

import pyspark.sql.functions as F
from pyspark.sql import Column, DataFrame

from databricks.labs.dqx import col_functions
from databricks.labs.dqx.utils import get_column_name


# TODO: this should perhaps be configurable
class Columns(Enum):
    ERRORS = "_errors"
    WARNINGS = "_warnings"


class Criticality(Enum):
    WARN = "warn"
    ERROR = "error"


@dataclass(frozen=True)
class DQRule:
    """Class to represent a data quality rule consisting of following fields:
    * `check` - Column expression to evaluate. This expression should return string value if it's evaluated to true -
    it will be used as an error/warning message, or `null` if it's evaluated to `false`
    * `name` - name that will be given to a resulting column. Autogenerated if not provided
    * `criticality` (optional) - possible values are `error` (critical problems), and `warn` (potential problems)
    """

    check: Column
    name: str = ""
    criticality: str = Criticality.ERROR.value

    def __post_init__(self):
        # take the name from the alias of the column expression if not provided
        object.__setattr__(self, "name", self.name if self.name else "col_" + get_column_name(self.check))

    @ft.cached_property
    def rule_criticality(self) -> Criticality:
        """Returns criticality of the check.

        :return: string describing criticality - `warn` or `error`. Raises exception if it's something else
        """
        _criticality = self.criticality
        if _criticality not in (Criticality.WARN.value, Criticality.ERROR.value):
            return Criticality.ERROR

        return Criticality(_criticality)

    @ft.cached_property
    def column_name(self) -> str:
        """Creates a column name based on the name specified in check plus criticality of the check.

        :return: name of the columns
        """
        return self.name + "_" + self.rule_criticality.value

    def check_column(self) -> Column:
        """Creates a Column object from the given check.

        :return: Column object
        """
        return (F.when(self.check.isNull(), F.lit(None).cast("string")).otherwise(self.check)).alias(self.column_name)


@dataclass(frozen=True)
class DQRuleColSet:
    """Class to represent a data quality col rule set which defines quality check function for a set of columns.
    The class consists of the following fields:
    * `columns` - list of column names to which the given check function should be applied
    * `criticality` - criticality level ('warn' or 'error')
    * `check_func` - check function to be applied
    * `check_func_args` - non-keyword / positional arguments for the check function after the col_name
    * `check_func_kwargs` - keyword /named arguments for the check function after the col_name
    """

    columns: list[str]
    check_func: Callable
    criticality: str = "error"
    check_func_args: list[Any] = field(default_factory=list)
    check_func_kwargs: dict[str, Any] = field(default_factory=dict)

    def get_rules(self) -> list[DQRule]:
        """Build a list of rules for a set of columns.

        :return: list of dq rules
        """
        return [
            DQRule(
                criticality=self.criticality,
                check=self.check_func(col_name, *self.check_func_args, **self.check_func_kwargs),
            )
            for col_name in self.columns
        ]


def _perform_checks(df: DataFrame, checks: list[DQRule]) -> DataFrame:
    """Applies a list of checks to a given dataframe and append results at the end of the dataframe.

    :param df: dataframe to check
    :param checks: list of DQRule objects describing the check
    :return: dataframe with check result columns added
    """
    checks_cols = [check.check_column() for check in checks]
    return df.select("*", *checks_cols)


def _make_null_filter(cols: list[str]) -> Column:
    """Creates a filter condition that check if all specified columns are null.

    :param cols: names of the columns to check
    :return: filter condition
    """
    if len(cols) == 0:
        return F.lit(True)

    def update_nullability_func(func, col):
        return func & F.col(col).isNull()

    initial = F.col(cols[0]).isNull()
    return ft.reduce(update_nullability_func, cols[1:], initial)


remove_criticality_re = re.compile("^(.*)_(error|warn)$")


def _with_checks_as_map(df: DataFrame, dest_col: str, cols: list[str]) -> DataFrame:
    """Collect individual check columns into corresponding map<string, string> errors or warnings columns.

    :param df: dataframe with added check columns of type map<string, string>
    :param dest_col: name of the map column (errors or warnings)
    :param cols: column names to put into the map
    :return:
    """
    null_map = F.lit(None).cast("map<string, string>")

    if len(cols) == 0:
        return df.withColumn(dest_col, null_map)

    map_cols = [
        F.struct(
            # put key as column name after removing criticality from it (warn or error)
            F.lit(re.sub(remove_criticality_re, "\\1", col)).alias("key"),
            F.col(col).cast("string").alias("value"),
        )
        for col in cols
    ]

    ndf = df.withColumn(dest_col, F.map_from_entries(F.array(*map_cols))).drop(*cols)

    # take only check results that are not null
    ndf = ndf.withColumn(dest_col, F.expr(f"map_filter({dest_col}, (k, v) -> v is not null)"))

    cols_without_checks = [F.col(nm) for nm in df.columns if nm not in cols]
    ndf = ndf.select(
        *cols_without_checks,
        # return null if no warnings / errors are present
        F.when(F.expr(f"exists(map_values({dest_col}), v -> v is not null)"), F.col(dest_col))
        .otherwise(null_map)
        .alias(dest_col),
    )

    return ndf


def _get_check_columns(checks: list[DQRule], criticality: Criticality) -> list[str]:
    """Get check columns based on criticality.

    :param checks: list of checks to apply to the dataframe
    :param criticality: criticality
    :return: list of check columns
    """
    return [check.column_name for check in checks if check.rule_criticality == criticality]


def _append_empty_checks(df: DataFrame) -> DataFrame:
    """Append empty checks at the end of dataframe.

    :param df: dataframe without checks
    :return: dataframe with checks
    """
    return df.select(
        "*",
        F.lit(None).cast("map<string, string>").alias(Columns.ERRORS.value),
        F.lit(None).cast("map<string, string>").alias(Columns.WARNINGS.value),
    )


def apply_checks(df: DataFrame, checks: list[DQRule]) -> DataFrame:
    """Applies data quality checks to a given dataframe.

    :param df: dataframe to check
    :param checks: list of checks to apply to the dataframe. Each check is an instance of DQRule class.
    :return: dataframe with errors and warning reporting columns
    """
    if not checks or len(checks) == 0:
        return df

    checked_df = _perform_checks(df, checks)

    warning_columns = _get_check_columns(checks, Criticality.WARN)
    error_columns = _get_check_columns(checks, Criticality.ERROR)

    checked_df_map = _with_checks_as_map(checked_df, Columns.ERRORS.value, error_columns)
    checked_df_map = _with_checks_as_map(checked_df_map, Columns.WARNINGS.value, warning_columns)

    checked_df_map = checked_df_map.select(*df.columns, Columns.ERRORS.value, Columns.WARNINGS.value)

    return checked_df_map


def apply_checks_and_split(df: DataFrame, checks: list[DQRule]) -> tuple[DataFrame, DataFrame]:
    """Applies data quality checks to a given dataframe and split it into two ("good" and "bad"),
    according to the data quality checks.

    :param df: dataframe to check
    :param checks: list of checks to apply to the dataframe. Each check is an instance of DQRule class.
    :return: two dataframes - "good" which includes warning rows but no reporting columns, and "data" having
    error and warning rows and corresponding reporting columns
    """
    if not checks or len(checks) == 0:
        return df, _append_empty_checks(df).limit(0)

    checked_df = apply_checks(df, checks)

    good_df = checked_df.where(F.col(Columns.ERRORS.value).isNull()).drop(Columns.ERRORS.value, Columns.WARNINGS.value)
    bad_df = checked_df.where(F.col(Columns.ERRORS.value).isNotNull() | F.col(Columns.WARNINGS.value).isNotNull())

    return good_df, bad_df


def build_checks_by_metadata(checks: list[dict], glbs: dict[str, Any] | None = None) -> list[DQRule]:
    """Build checks based on check specification, i.e. function name plus arguments.

    :param checks: list of dictionaries describing checks. Each check is a dictionary consisting of following fields:
    * `check` - Column expression to evaluate. This expression should return string value if it's evaluated to true -
    it will be used as an error/warning message, or `null` if it's evaluated to `false`
    * `name` - name that will be given to a resulting column. Autogenerated if not provided
    * `criticality` (optional) - possible values are `error` (data going only into "bad" dataframe),
    and `warn` (data is going into both dataframes)
    :param glbs: dictionary with functions mapping (eg. ``globals()`` of the calling module).
    If not specified, then only built-in functions are used for the checks.
    :return: list of data quality check rules
    """
    dq_rule_checks = []
    for check_def in checks:
        check = check_def.get("check")
        if not check:
            raise ValueError(f"'check' block should be provided in the check: {check}")

        func_name = check.get("function")
        if not func_name:
            raise ValueError(f"'function' argument should be provided in the check: {check}")

        if glbs:
            func = glbs.get(func_name)
        else:
            func = getattr(col_functions, func_name)

        if not func or not callable(func):
            raise ValueError(f"function {func_name} is not defined")

        func_args = check.get("arguments", {})
        criticality = check_def.get("criticality", "error")

        if "col_names" in func_args:
            dq_rule_checks += DQRuleColSet(
                columns=func_args["col_names"],
                check_func=func,
                criticality=criticality,
                # provide arguments without "col_names"
                check_func_kwargs={k: func_args[k] for k in func_args.keys() - {"col_names"}},
            ).get_rules()
        else:
            name = check_def.get("name", None)
            check_func = func(**func_args)
            dq_rule_checks.append(DQRule(check=check_func, name=name, criticality=criticality))

    return dq_rule_checks


def apply_checks_by_metadata_and_split(
    df: DataFrame, checks: list[dict], glbs: dict[str, Any] | None = None
) -> tuple[DataFrame, DataFrame]:
    """Wrapper around `apply_checks_and_split` for use in the metadata-driven pipelines. The main difference
    is how the checks are specified - instead of using functions directly, they are described as function name plus
    arguments.

    :param df: dataframe to check
    :param checks: list of dictionaries describing checks. Each check is a dictionary consisting of following fields:
    * `check` - Column expression to evaluate. This expression should return string value if it's evaluated to true -
    it will be used as an error/warning message, or `null` if it's evaluated to `false`
    * `name` - name that will be given to a resulting column. Autogenerated if not provided
    * `criticality` (optional) - possible values are `error` (data going only into "bad" dataframe),
    and `warn` (data is going into both dataframes)
    :param glbs: dictionary with functions mapping (eg. ``globals()`` of the calling module).
    If not specified, then only built-in functions are used for the checks.
    :return: two dataframes - "good" which includes warning rows but no reporting columns, and "bad" having
    error and warning rows and corresponding reporting columns
    """
    dq_rule_checks = build_checks_by_metadata(checks, glbs)

    good_df, bad_df = apply_checks_and_split(df, dq_rule_checks)

    return good_df, bad_df


def apply_checks_by_metadata(df: DataFrame, checks: list[dict], glbs: dict[str, Any] | None = None) -> DataFrame:
    """Wrapper around `apply_checks` for use in the metadata-driven pipelines. The main difference
    is how the checks are specified - instead of using functions directly, they are described as function name plus
    arguments.

    :param df: dataframe to check
    :param checks: list of dictionaries describing checks. Each check is a dictionary consisting of following fields:
    * `check` - Column expression to evaluate. This expression should return string value if it's evaluated to true -
    it will be used as an error/warning message, or `null` if it's evaluated to `false`
    * `name` - name that will be given to a resulting column. Autogenerated if not provided
    * `criticality` (optional) - possible values are `error` (data going only into "bad" dataframe),
    and `warn` (data is going into both dataframes)
    :param glbs: dictionary with functions mapping (eg. ``globals()`` of calling module).
    If not specified, then only built-in functions are used for the checks.
    :return: dataframe with errors and warning reporting columns
    """
    dq_rule_checks = build_checks_by_metadata(checks, glbs)

    return apply_checks(df, dq_rule_checks)


def build_checks(*rules_col_set: DQRuleColSet) -> list[DQRule]:
    """
    Build rules from dq rules and rule sets.

    :param rules_col_set: list of dq rules which define multiple columns for the same check function
    :return: list of dq rules
    """
    rules_nested = [rule_set.get_rules() for rule_set in rules_col_set]
    flat_rules = list(itertools.chain(*rules_nested))

    return list(filter(None, flat_rules))
